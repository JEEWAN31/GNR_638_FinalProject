{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  # %tensorflow_version always exist in colab\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass"
      ],
      "metadata": {
        "id": "JUbTQtxAPhzd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "id": "oWVDvvlRs3-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "DIV2K_RGB_MEAN = np.array([0.4488, 0.4371, 0.4040]) * 255\n",
        "\n",
        "\n",
        "def resolve_single(model, lr):\n",
        "    return resolve(model, tf.expand_dims(lr, axis=0))[0]\n",
        "\n",
        "\n",
        "def resolve(model, lr_batch):\n",
        "    lr_batch = tf.cast(lr_batch, tf.float32)\n",
        "    sr_batch = model(lr_batch)\n",
        "    sr_batch = tf.clip_by_value(sr_batch, 0, 255)\n",
        "    sr_batch = tf.round(sr_batch)\n",
        "    sr_batch = tf.cast(sr_batch, tf.uint8)\n",
        "    return sr_batch\n",
        "\n",
        "\n",
        "\n",
        "def normalize(x, rgb_mean=DIV2K_RGB_MEAN):\n",
        "    return (x - rgb_mean) / 127.5\n",
        "\n",
        "\n",
        "def denormalize(x, rgb_mean=DIV2K_RGB_MEAN):\n",
        "    return x * 127.5 + rgb_mean\n",
        "\n",
        "\n",
        "def normalize_01(x):\n",
        "    \"\"\"Normalizes RGB images to [0, 1].\"\"\"\n",
        "    return x / 255.0\n",
        "\n",
        "\n",
        "def normalize_m11(x):\n",
        "    \"\"\"Normalizes RGB images to [-1, 1].\"\"\"\n",
        "    return x / 127.5 - 1\n",
        "\n",
        "\n",
        "def denormalize_m11(x):\n",
        "    \"\"\"Inverse of normalize_m11.\"\"\"\n",
        "    return (x + 1) * 127.5\n",
        "\n",
        "\n",
        "\n",
        "def pixel_shuffle(scale):\n",
        "    return lambda x: tf.nn.depth_to_space(x, scale)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7KvId9o3pLJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Add, BatchNormalization, Conv2D, Dense, Flatten, Input, LeakyReLU, PReLU, Lambda\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.applications.vgg19 import VGG19\n",
        "\n",
        "LR_SIZE = 24\n",
        "HR_SIZE = 96\n",
        "\n",
        "\n",
        "def upsample(x_in, num_filters):\n",
        "    x = Conv2D(num_filters, kernel_size=3, padding='same')(x_in)\n",
        "    x = Lambda(pixel_shuffle(scale=2))(x)           #Upsampling with a size of 2         \n",
        "    return PReLU(shared_axes=[1, 2])(x)\n",
        "\n",
        "\n",
        "def res_block(x_in, num_filters, momentum=0.8):\n",
        "    x = Conv2D(num_filters, kernel_size=3, padding='same')(x_in)\n",
        "    x = BatchNormalization(momentum=momentum)(x)\n",
        "    x = PReLU(shared_axes=[1, 2])(x)\n",
        "\n",
        "    x = Conv2D(num_filters, kernel_size=3, padding='same')(x)\n",
        "    x = BatchNormalization(momentum=momentum)(x)\n",
        "    x = Add()([x_in, x])\n",
        "    return x\n",
        "\n",
        "\n",
        "def sr_resnet(num_filters=64, num_res_blocks=16):\n",
        "    x_in = Input(shape=(LR_SIZE, LR_SIZE, 3))\n",
        "    x = Lambda(normalize_01)(x_in)\n",
        "\n",
        "    x = Conv2D(num_filters, kernel_size=9, padding='same')(x)\n",
        "    x = x_1 = PReLU(shared_axes=[1, 2])(x)\n",
        "\n",
        "    for i in range(num_res_blocks):\n",
        "        x = res_block(x, num_filters)\n",
        "\n",
        "    x = Conv2D(num_filters, kernel_size=3, padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Add()([x_1, x])\n",
        "\n",
        "    x = upsample(x, num_filters * 4)\n",
        "    x = upsample(x, num_filters * 4)\n",
        "\n",
        "    x = Conv2D(3, kernel_size=9, padding='same', activation='tanh')(x)\n",
        "    x = Lambda(denormalize_m11)(x)\n",
        "\n",
        "    return Model(x_in, x)\n",
        "\n",
        "\n",
        "generator = sr_resnet\n",
        "\n",
        "\n",
        "def discriminator_block(x_in, num_filters, strides=1, batchnorm=True, momentum=0.8):\n",
        "    x = Conv2D(num_filters, kernel_size=3, strides=strides, padding='same')(x_in)\n",
        "    if batchnorm:\n",
        "        x = BatchNormalization(momentum=momentum)(x)\n",
        "    return LeakyReLU(alpha=0.2)(x)\n",
        "\n",
        "\n",
        "def discriminator(num_filters=64):\n",
        "    x_in = Input(shape=(HR_SIZE, HR_SIZE, 3))\n",
        "    x = Lambda(normalize_m11)(x_in)\n",
        "\n",
        "    x = discriminator_block(x, num_filters, batchnorm=False)\n",
        "    x = discriminator_block(x, num_filters, strides=2)\n",
        "\n",
        "    x = discriminator_block(x, num_filters * 2)\n",
        "    x = discriminator_block(x, num_filters * 2, strides=2)\n",
        "\n",
        "    x = discriminator_block(x, num_filters * 4)\n",
        "    x = discriminator_block(x, num_filters * 4, strides=2)\n",
        "\n",
        "    x = discriminator_block(x, num_filters * 8)\n",
        "    x = discriminator_block(x, num_filters * 8, strides=2)\n",
        "\n",
        "    x = Flatten()(x)\n",
        "\n",
        "    x = Dense(1024)(x)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "    x = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    return Model(x_in, x)\n"
      ],
      "metadata": {
        "id": "FZpV4H1QpNky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator().summary()\n",
        "discriminator().summary()\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(generator())\n",
        "model.add(discriminator())\n",
        "model.summary()\n",
        "discriminator().compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\")\n",
        "discriminator().trainable = False\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\")"
      ],
      "metadata": {
        "id": "xJXEFwWjpzUG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "def train_dcgan(model,epochs=5):\n",
        "\n",
        "    print(\"done\")\n",
        "    generator, discriminator = model.layers\n",
        "    discriminator.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\")\n",
        "    discriminator.trainable = False\n",
        "    model.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\")\n",
        "    path = '/content/drive/MyDrive/cars_train/'\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "      \n",
        "      print(\"Epoch {}/{}\".format(epoch + 1, epochs))\n",
        "      for root, dirnames, filenames in os.walk(path):\n",
        "        i = 0\n",
        "        j = 0\n",
        "        x_train_x = np.zeros((32,24,24,3))\n",
        "        x_train_y = np.zeros((32,96,96,3))\n",
        "        for filename in filenames:\n",
        "          img_path = os.path.join(path,filename)\n",
        "          x_train = cv2.imread(img_path)\n",
        "          x_trainx = cv2.resize(x_train,(24,24))\n",
        "          x_trainy = cv2.resize(x_train,(96,96))\n",
        "          x_train_x[i] = x_trainx \n",
        "          x_train_y[i] = x_trainy\n",
        "          i = i+1\n",
        "          if i == 32:\n",
        "            j = j + 1\n",
        "            print(\"batch {}/254\".format(j))\n",
        "            X_batch, Y_batch = x_train_x, x_train_y\n",
        "            X_batch = tf.cast(X_batch, tf.float32)\n",
        "            Y_batch = tf.cast(Y_batch, tf.float32)\n",
        "            generated_images = generator(X_batch)\n",
        "            X = tf.cast(generated_images, tf.float32) \n",
        "            X_fake_and_real = tf.concat([X, Y_batch], axis=0)\n",
        "            y1 = tf.constant([[0.]] * batch_size + [[1.]] * batch_size)\n",
        "            discriminator.trainable = True\n",
        "            history_disc = discriminator.train_on_batch(X_fake_and_real, y1)\n",
        "            y2 = tf.constant([[1.]] * batch_size)\n",
        "            discriminator.trainable = False\n",
        "            history_gen = model.train_on_batch(X_batch, y2)\n",
        "            i = 0\n",
        "            x_train_x = np.zeros((32,24,24,3))\n",
        "            x_train_y = np.zeros((32,96,96,3))\n",
        "    \n",
        "      if (epoch+1)%10 == 0:\n",
        "\n",
        "        generator.save_weights(\"Generator{}.h5\".format(epoch))\n",
        "        discriminator.save_weights(\"Discriminator_weights{}.h5\".format(epoch))\n",
        "        model.save_weights(\"Model{}.h5\".format(epoch))\n",
        "        from google.colab.patches import cv2_imshow\n",
        "\n",
        "        path = \"/content/drive/MyDrive/cars_train/07336.jpg\"\n",
        "\n",
        "        X = cv2.imread(path)\n",
        "        X = cv2.resize(X,(24,24))\n",
        "        X = np.reshape(X, (1,24,24,3))\n",
        "        X_batch = tf.cast(X, tf.float32)\n",
        "\n",
        "        generator.load_weights(\"Generator{}.h5\".format(epoch))\n",
        "        discriminator.load_weights(\"Discriminator_weights{}.h5\".format(epoch))\n",
        "        Y = generator(X_batch)\n",
        "        cv2_imshow(X[0])\n",
        "        cv2_imshow( Y[0].numpy())"
      ],
      "metadata": {
        "id": "GVoHgB72qedS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 2200\n",
        "train_dcgan(model,epochs=epochs)"
      ],
      "metadata": {
        "id": "DfzT8If0qk6w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "path = \"/content/drive/MyDrive/cars_train/04336.jpg\"\n",
        "\n",
        "X2 = cv2.imread(path)\n",
        "X1 = cv2.resize(X2,(24,24), interpolation = cv2.INTER_AREA)\n",
        "X = np.reshape(X1, (1,24,24,3))\n",
        "X_batch = tf.cast(X, tf.float32)\n",
        "\n",
        "generator, discriminator = model.layers\n",
        "generator.load_weights(\"/content/gan_generator.h5\")\n",
        "Y = generator(X_batch)\n",
        "cv2_imshow(X[0])\n",
        "cv2_imshow( Y[0].numpy())"
      ],
      "metadata": {
        "id": "pz0_QKsXqoEl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}